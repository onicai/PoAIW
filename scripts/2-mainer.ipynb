{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Verify we're in the Conda environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "\n",
                "print(sys.executable)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import python packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "from PIL import Image\n",
                "import base64\n",
                "import io\n",
                "from dotenv import load_dotenv\n",
                "import requests\n",
                "import pprint\n",
                "from pathlib import Path\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "import subprocess\n",
                "import jupyter_black\n",
                "\n",
                "# Activate the jupyter_black extension, which reformats code cells with black\n",
                "# https://github.com/n8henrie/jupyter-black\n",
                "jupyter_black.load()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import hashlib\n",
                "\n",
                "\n",
                "def calculate_hash_seed(sentence, num_try):\n",
                "    # Encode the sentence to bytes\n",
                "    sentence_bytes = sentence.encode(\"utf-8\")\n",
                "\n",
                "    # Create a hash object\n",
                "    hash_object = hashlib.sha256()\n",
                "\n",
                "    # Update the hash object with the bytes\n",
                "    hash_object.update(sentence_bytes)\n",
                "\n",
                "    # Get the hexadecimal representation of the hash\n",
                "    hash_hex = hash_object.hexdigest()\n",
                "\n",
                "    # Convert the hexadecimal hash to a decimal number\n",
                "    hash_decimal = int(hash_hex, 16)\n",
                "\n",
                "    # Make it more random by multiplying by the number of tries\n",
                "    hash_decimal *= num_try * 100\n",
                "\n",
                "    # Limit the hash value to the range of a 32-bit signed integer\n",
                "    seed = hash_decimal % (2**31 - 1)\n",
                "\n",
                "    return seed\n",
                "\n",
                "\n",
                "# Example usage\n",
                "sentence = \"This is a sample sentence.\"\n",
                "seed_value = calculate_hash_seed(sentence, 1)\n",
                "print(f\"The hash of the sentence as a seed is: {seed_value}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "\n",
                "def filter_text(text):\n",
                "    # Only keep the first line of the answer\n",
                "    text = text.split(\"\\n\")[0]\n",
                "\n",
                "    # Remove quotes from the answer. Both single and double quotes are removed.\n",
                "    text = text.replace('\"', \"\").replace(\"'\", \"\")\n",
                "\n",
                "    # Remove leading and trailing whitespaces\n",
                "    text = text.strip()\n",
                "\n",
                "    # Use regex to remove special characters\n",
                "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
                "\n",
                "    # Remove backslash from the answer.\n",
                "    text = text.replace(\"\\\\\", \"\")\n",
                "\n",
                "    # Remove [end of text]\n",
                "    text = text.replace(\"[end of text]\", \"\")\n",
                "\n",
                "    return text\n",
                "\n",
                "\n",
                "# Example usage\n",
                "text = \"Byzantine\\u5171\\u8bc6 ('Byzantine' \\\"consensus\\\")\\n next line must be removed [end of text]\"\n",
                "filtered_text = filter_text(text)\n",
                "print(filtered_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define where the llama-cli is located, relative to this notebook\n",
                "LLAMA_CLI_PATH = \"../../../ggerganov_llama_615212.cpp/build/bin/llama-cli\"\n",
                "# LLAMA_CLI_PATH = \"../../../ggerganov_llama_latest.cpp/build/bin/llama-cli\"\n",
                "\n",
                "# Select a model to use\n",
                "MODEL = \"../../../llama_cpp_canister/models/Qwen/Qwen2.5-0.5B-Instruct-GGUF/qwen2.5-0.5b-instruct-q8_0.gguf\"\n",
                "# MODEL = \"../../../llama_cpp_canister/models/tensorblock/SmolLM2-135M-Instruct-GGUF/SmolLM2-135M-Instruct-Q8_0.gguf\"\n",
                "# MODEL = (\n",
                "#     \"../../../llama_cpp_canister/models/tensorblock/SmolLM2-135M-Instruct-GGUF/SmolLM2-135M-Instruct-Q4_K_M.gguf\"\n",
                "# )\n",
                "# MODEL = \"../../../llama_cpp_canister/models/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf\"\n",
                "# MODEL = \"../../../llama_cpp_canister/models/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B-Q2_K.gguf\"\n",
                "\n",
                "print_command = True\n",
                "\n",
                "\n",
                "def run_llama_cpp(prompt, num_tokens, seed, temp, top_k, top_p, min_p):\n",
                "    command = [\n",
                "        LLAMA_CLI_PATH,\n",
                "        \"-m\",\n",
                "        MODEL,\n",
                "        \"--no-warmup\",  # needed when running from CLI. Is default for llama_cpp_canister\n",
                "        \"-no-cnv\",  # needed when running from CLI. Is default for llama_cpp_canister\n",
                "        \"--simple-io\",\n",
                "        \"--no-display-prompt\",  # only return the generated text, without special characters\n",
                "        # \"-sp\", # output special tokens\n",
                "        \"-n\",\n",
                "        f\"{num_tokens}\",\n",
                "        \"--seed\",\n",
                "        f\"{seed}\",\n",
                "        \"--temp\",\n",
                "        f\"{temp}\",\n",
                "        \"--top-k\",\n",
                "        f\"{top_k}\",\n",
                "        \"--top-p\",\n",
                "        f\"{top_p}\",\n",
                "        \"--min-p\",\n",
                "        f\"{min_p}\",\n",
                "        \"-p\",\n",
                "        prompt,\n",
                "    ]\n",
                "\n",
                "    # print this only once !\n",
                "    global print_command\n",
                "    if print_command:\n",
                "        print_command = False\n",
                "        # Print the command on a single line for terminal use, preserving \\n\n",
                "        print(\n",
                "            \"\\nCommand:\\n\",\n",
                "            f\"{LLAMA_CLI_PATH} -m {MODEL} --no-warmup -no-cnv --simple-io --no-display-prompt -n {num_tokens} --seed {seed} --temp {temp} -p '{prompt}'\".replace(\n",
                "                \"\\n\", \"\\\\n\"\n",
                "            ),\n",
                "        )\n",
                "\n",
                "    # Run the command and capture the output\n",
                "    result = subprocess.run(\n",
                "        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
                "    )\n",
                "    output = result.stdout\n",
                "    return output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import pprint\n",
                "\n",
                "# Specify the file path\n",
                "file_path = \"1-challenges.json\"\n",
                "\n",
                "# Read the list from the JSON file\n",
                "with open(file_path, \"r\") as file:\n",
                "    challenges = json.load(file)\n",
                "\n",
                "print(f\"Challenges have been read from the file: {file_path}\")\n",
                "\n",
                "pprint.pprint(challenges[:3])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "mainer_ids = [\n",
                "    \"4hfoe-6aaaa-aaaaj-qnfwq-cai\",\n",
                "    \"4ogfy-iiaaa-aaaaj-qnfxa-cai\",\n",
                "    \"4jhdm-fqaaa-aaaaj-qnfxq-cai\",\n",
                "    \"6u2ne-wyaaa-aaaaj-qnfya-cai\",\n",
                "    \"lkh5o-3yaaa-aaaag-acguq-cai\",\n",
                "    \"laxfu-oqaaa-aaaag-ak5kq-cai\",\n",
                "    \"ljuoi-yyaaa-aaaag-ak5la-cai\",\n",
                "    \"lovi4-vaaaa-aaaag-ak5lq-cai\",\n",
                "    \"tsigi-iyaaa-aaaaj-azxea-cai\",\n",
                "    \"tvja4-faaaa-aaaaj-azxeq-cai\",\n",
                "    \"t4kla-tiaaa-aaaaj-azxfa-cai\",\n",
                "    \"t3lnu-6qaaa-aaaaj-azxfq-cai\",\n",
                "]\n",
                "\n",
                "# Initial seeds\n",
                "# mainer_seeds = [calculate_hash_seed(mainer_id) for mainer_id in mainer_ids]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "challenges_with_answers = challenges.copy()\n",
                "\n",
                "mainer_num_tokens = 20\n",
                "\n",
                "num_tries = 10\n",
                "\n",
                "for challenge in challenges_with_answers:\n",
                "    challenge_id = challenge[\"challenge_id\"]\n",
                "    challenge_topic = challenge[\"challenge_topic\"]\n",
                "    challenge_question = challenge[\"challenge_question\"]\n",
                "\n",
                "    # Dynamically add the mainer_answers key to the challenge dictionary\n",
                "    # (Envisioning that the caller to the mainers will do it like this...)\n",
                "    challenge[\"mainer_answers\"] = []\n",
                "\n",
                "    print(\"-------------------\")\n",
                "    print(\n",
                "        f\"challenge_id: {challenge_id}, topic: {challenge_topic} - challenge_question: {challenge_question}\"\n",
                "    )\n",
                "\n",
                "    mainer_top_k = 40  # (default: 40, 0 = disabled)\n",
                "    mainer_top_p = 0.9  # (default: 0.9, 1.0 = disabled)\n",
                "    mainer_min_p = 0.1  # (default: 0.1, 0.0 = disabled)\n",
                "    for i, mainer_id in enumerate(mainer_ids):\n",
                "        for mainer_temp in [\n",
                "            # 3.0,\n",
                "            # 2.5,\n",
                "            # 2.0,\n",
                "            # 1.75,\n",
                "            # 1.5,\n",
                "            # 1.25,\n",
                "            # 1.0,\n",
                "            0.7,\n",
                "            # 0.6,\n",
                "            # 0.4,\n",
                "            # 0.2,\n",
                "            # 0.0,\n",
                "        ]:\n",
                "            # for mainer_top_p in [\n",
                "            #     1.0,\n",
                "            #     # 0.9, 0.8, 0.6, 0.4, 0.2\n",
                "            # ]:\n",
                "\n",
                "            # mainer_prompt = f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nThis is a question about {challenge_topic}, that can be answered with common knowledge. Give the answer as brief as possible. This is the question: {challenge_question}\\n<|im_end|>\\n<|im_start|>assistant\\nThe answer is:\"\n",
                "            mainer_prompt = f\"<|im_start|>user\\nThis is a question about {challenge_topic}. Give the answer as brief as possible. This is the question: {challenge_question}\\n<|im_end|>\\n<|im_start|>assistant\\nThe answer is:\"\n",
                "            # mainer_prompt = f\"<|im_start|>user\\nAnswer this question as brief as possible.\\nThis is the question: {challenge_question}\\n<|im_end|>\\n<|im_start|>assistant\\nThe answer is:\"\n",
                "\n",
                "            for mainer_try in range(num_tries):\n",
                "                # mainer_seed = mainer_seeds[i] * (mainer_try +1)\n",
                "                if mainer_try < 3:\n",
                "                    mainer_seed = i * (mainer_try + 1) * 10 + i + mainer_try\n",
                "                elif mainer_try < 6:\n",
                "                    mainer_seed = (\n",
                "                        (i + 1) * (mainer_try + 1) * 100\n",
                "                        + i * (mainer_try + 1) * 10\n",
                "                        + i\n",
                "                        + mainer_try\n",
                "                    )\n",
                "                else:\n",
                "                    mainer_seed = calculate_hash_seed(mainer_id, mainer_try)\n",
                "                # mainer_seed = -1\n",
                "                mainer_answer = run_llama_cpp(\n",
                "                    mainer_prompt,\n",
                "                    mainer_num_tokens,\n",
                "                    mainer_seed,\n",
                "                    mainer_temp,\n",
                "                    mainer_top_k,\n",
                "                    mainer_top_p,\n",
                "                    mainer_min_p,\n",
                "                )\n",
                "\n",
                "                mainer_answer = filter_text(mainer_answer)\n",
                "                challenge[\"mainer_answers\"].append(\n",
                "                    {\n",
                "                        \"mainer_id\": mainer_id,\n",
                "                        \"mainer_try\": mainer_try,\n",
                "                        \"mainer_seed\": mainer_seed,\n",
                "                        \"mainer_answer\": mainer_answer,\n",
                "                        \"mainer_temp\": mainer_temp,\n",
                "                        \"mainer_top_p\": mainer_top_p,\n",
                "                    }\n",
                "                )\n",
                "\n",
                "                print(\n",
                "                    f\"mainer_id: {mainer_id}, mainer_try: {mainer_try}, mainer_seed: {mainer_seed}, mainer_temp: {mainer_temp}, mainer_top_p: {mainer_top_p}, mainer_answer: {mainer_answer}\"\n",
                "                )\n",
                "\n",
                "                # calculate seed for next iteration\n",
                "                # mainer_seeds[i] = calculate_hash_seed(\n",
                "                #     str(mainer_seed) + mainer_id + mainer_answer\n",
                "                # )\n",
                "                # if mainer_seeds[i] == mainer_seed:\n",
                "                #     mainer_seeds[i] = calculate_hash_seed(\n",
                "                #         str(mainer_seed)\n",
                "                #         + mainer_id\n",
                "                #         + mainer_answer\n",
                "                #         + str(mainer_seed)\n",
                "                #         + mainer_id\n",
                "                #         + mainer_answer\n",
                "                #     )\n",
                "\n",
                "                # quick check to see data is stored correctly\n",
                "                # pprint.pprint(challenges_with_answers[:3])\n",
                "\n",
                "                # On a regular basis, save it, so we can monitor progress\n",
                "                # Specify the file path\n",
                "                file_path = \"2-mainer.json\"\n",
                "\n",
                "                # Write the list to a JSON file\n",
                "                with open(file_path, \"w\") as file:\n",
                "                    json.dump(challenges_with_answers, file, indent=4)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "PoAIW",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
