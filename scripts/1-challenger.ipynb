{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Verify we're in the Conda environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "\n",
                "print(sys.executable)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import python packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "from PIL import Image\n",
                "import base64\n",
                "import io\n",
                "from dotenv import load_dotenv\n",
                "import requests\n",
                "import pprint\n",
                "from pathlib import Path\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "import subprocess\n",
                "import jupyter_black\n",
                "\n",
                "# Activate the jupyter_black extension, which reformats code cells with black\n",
                "# https://github.com/n8henrie/jupyter-black\n",
                "jupyter_black.load()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import hashlib\n",
                "\n",
                "\n",
                "def calculate_hash_seed(sentence):\n",
                "    # Encode the sentence to bytes\n",
                "    sentence_bytes = sentence.encode(\"utf-8\")\n",
                "\n",
                "    # Create a hash object\n",
                "    hash_object = hashlib.sha256()\n",
                "\n",
                "    # Update the hash object with the bytes\n",
                "    hash_object.update(sentence_bytes)\n",
                "\n",
                "    # Get the hexadecimal representation of the hash\n",
                "    hash_hex = hash_object.hexdigest()\n",
                "\n",
                "    # Convert the hexadecimal hash to a decimal number\n",
                "    hash_decimal = int(hash_hex, 16)\n",
                "\n",
                "    # Limit the hash value to the range of a 32-bit signed integer\n",
                "    seed = hash_decimal % (2**31 - 1)\n",
                "\n",
                "    return seed\n",
                "\n",
                "\n",
                "# Example usage\n",
                "sentence = \"This is a sample sentence.\"\n",
                "seed_value = calculate_hash_seed(sentence)\n",
                "print(f\"The hash of the sentence as a seed is: {seed_value}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "\n",
                "def filter_text(text):\n",
                "    # Only keep the first line of the answer\n",
                "    text = text.split(\"\\n\")[0]\n",
                "\n",
                "    # Remove quotes from the answer. Both single and double quotes are removed.\n",
                "    text = text.replace('\"', \"\").replace(\"'\", \"\")\n",
                "\n",
                "    # Remove leading and trailing whitespaces\n",
                "    text = text.strip()\n",
                "\n",
                "    # Use regex to remove special characters\n",
                "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
                "\n",
                "    # Remove backslash from the answer.\n",
                "    text = text.replace(\"\\\\\", \"\")\n",
                "\n",
                "    # Remove [end of text]\n",
                "    text = text.replace(\"[end of text]\", \"\")\n",
                "\n",
                "    return text\n",
                "\n",
                "\n",
                "# Example usage\n",
                "text = \"Byzantine\\u5171\\u8bc6 ('Byzantine' \\\"consensus\\\")\\n next line must be removed\"\n",
                "filtered_text = filter_text(text)\n",
                "print(filtered_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define where the llama-cli is located, relative to this notebook\n",
                "LLAMA_CLI_PATH = \"../../../ggerganov_llama_615212.cpp/build/bin/llama-cli\"\n",
                "# LLAMA_CLI_PATH = \"../../../ggerganov_llama_latest.cpp/build/bin/llama-cli\"\n",
                "\n",
                "# Select a model to use\n",
                "MODEL = \"../../../llama_cpp_canister/models/Qwen/Qwen2.5-0.5B-Instruct-GGUF/qwen2.5-0.5b-instruct-q8_0.gguf\"\n",
                "# MODEL = \"../../../llama_cpp_canister/models/tensorblock/SmolLM2-135M-Instruct-GGUF/SmolLM2-135M-Instruct-Q8_0.gguf\"\n",
                "# MODEL = (\n",
                "#     \"../../../llama_cpp_canister/models/tensorblock/SmolLM2-135M-Instruct-GGUF/SmolLM2-135M-Instruct-Q4_K_M.gguf\"\n",
                "# )\n",
                "# MODEL = \"../../../llama_cpp_canister/models/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf\"\n",
                "# MODEL = \"../../../llama_cpp_canister/models/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B-Q2_K.gguf\"\n",
                "\n",
                "print_command = True\n",
                "def run_llama_cpp(\n",
                "    prompt,\n",
                "    num_tokens,\n",
                "    seed,\n",
                "    temp,\n",
                "    # top_k,\n",
                "    # top_p,\n",
                "    # min_p,\n",
                "):\n",
                "    command = [\n",
                "        LLAMA_CLI_PATH,\n",
                "        \"-m\",\n",
                "        MODEL,\n",
                "        \"--no-warmup\", # needed when running from CLI. Is default for llama_cpp_canister\n",
                "        \"-no-cnv\", # needed when running from CLI. Is default for llama_cpp_canister\n",
                "        \"--simple-io\",\n",
                "        \"--no-display-prompt\",  # only return the generated text, without special characters\n",
                "        # \"-sp\", # output special tokens\n",
                "        \"-n\",\n",
                "        f\"{num_tokens}\",\n",
                "        \"--seed\",\n",
                "        f\"{seed}\",\n",
                "        \"--temp\",\n",
                "        f\"{temp}\",\n",
                "        # \"--top-k\",\n",
                "        # f\"{top_k}\",\n",
                "        # \"--top-p\",\n",
                "        # f\"{top_p}\",\n",
                "        # \"--min-p\",\n",
                "        # f\"{min_p}\",\n",
                "        \"-p\",\n",
                "        prompt,\n",
                "    ]\n",
                "\n",
                "    # print this only once !\n",
                "    global print_command\n",
                "    if print_command:\n",
                "        print_command = False\n",
                "        # Print the command on a single line for terminal use, preserving \\n\n",
                "        print(\n",
                "            \"\\nCommand:\\n\",\n",
                "            f\"{LLAMA_CLI_PATH} -m {MODEL} --no-warmup -no-cnv --simple-io --no-display-prompt -n {num_tokens} --seed {seed} --temp {temp} -p '{prompt}'\".replace(\n",
                "                \"\\n\", \"\\\\n\"\n",
                "            ),\n",
                "        )\n",
                "\n",
                "    # Run the command and capture the output\n",
                "    result = subprocess.run(\n",
                "        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
                "    )\n",
                "    output = result.stdout\n",
                "    return output\n",
                "\n",
                "\n",
                "# create_challenge_prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGenerate a question that can be answered by an LLM in 5 words or less.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "# create_challenge_prompt = \"<|im_start|>user\\nGenerate a question that can be answered in 5 words or less.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "# BAD create_challenge_prompt = \"<|im_start|>user\\nPlease create a question that can be answered with common knowledge.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "# create_challenge_prompt = \"<|im_start|>system\\nYou are a quiz master.<|im_end|>\\n<|im_start|>user\\nGenerate 10 different questions that can be answered in 5 words or less. Return as JSON<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "# create_challenge_prompt = \"<|im_start|>system\\nYou are a quiz master.<|im_end|>\\n<|im_start|>user\\nAsk me 10 different questions that can be answered in 5 words or less. Return as JSON<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "# create_challenge_prompt = \"<|im_start|>system\\nYou are a quiz master.<|im_end|>\\n<|im_start|>user\\nAsk me 10 different questions that start with What, Who, Where, When, Why, How, Which, Can, Is, or Are. Also give me the answers. Return as JSON<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "# create_challenge_prompt = '<|im_start|>system\\nYou are a quiz master.<|im_end|>\\n<|im_start|>user\\nAsk me 10 different questions that start with What, Who, Where, When, Why, How, Which or Can. Also give me the answers. Return as a JSON format as shown in this example:\\n[{\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"}, {\"question\": \"Who first discovered the Americas?\", \"answer\": \"Christopher Columbus\"}, {\"question\": \"Where is the headquarters of NASA located?\", \"answer\": \"Houston, Texas\"}, {\"question\": \"When did the First World War start?\", \"answer\": \"1914\"}, {\"question\": \"What is the longest river in the world?\", \"answer\": \"Mississippis\"}, {\"question\": \"Which historical event is considered the turning point in the Civil War?\", \"answer\": \"The Battle of Gettysburg\"}, {\"question\": \"Which year was the first Olympics held?\", \"answer\": \"1896\"}, {\"question\": \"Which city in France was the birthplace of French President Fran√ßois Hollande?\", \"answer\": \"Paris\"}, {\"question\": \"What is the name of the largest planet in our solar system?\", \"answer\": \"Mercury\"}, {\"question\": \"Who is known as the \\'King of Jazz\\'?\", \"answer\": \"Louis Armstrong\"}]\\n<|im_end|>\\n<|im_start|>assistant\\n'\n",
                "# create_challenge_prompt = \"<|im_start|>system\\nYou are a quiz master.<|im_end|>\\n<|im_start|>user\\nAsk me a question that starts with What, Who, Where, When, Why, How, Which or Can. Do NOT give me the answer.\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "# create_challenge_prompt = \"<|im_start|>user\\nAsk a question that can be answered with common knowledge. Do NOT give the answer.\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "create_challenge_prompt_starts_with = [\n",
                "    \"What\",\n",
                "    \"Who\",\n",
                "    \"Where\",\n",
                "    \"When\",\n",
                "    \"Why\",\n",
                "    \"How\",\n",
                "    \"Which\",\n",
                "    \"Can\",\n",
                "]\n",
                "create_challenge_num_tokens = 1024\n",
                "\n",
                "create_challenge_temp = 0.7\n",
                "# create_challenge_top_k = 50\n",
                "# create_challenge_top_p = 0.95\n",
                "# create_challenge_min_p = 0.05\n",
                "\n",
                "create_challenge_seed = None\n",
                "# create_challenge_seed = calculate_hash_seed(create_challenge_prompt)\n",
                "# print(\n",
                "#     f\"create_challenge_prompt = {create_challenge_prompt} (seed = {create_challenge_seed})\"\n",
                "# )\n",
                "\n",
                "\n",
                "challenge_topics = [\n",
                "    \"crypto\",\n",
                "    \"nature\",\n",
                "    \"space\",\n",
                "    \"history\",\n",
                "    \"science\",\n",
                "    \"technology\",\n",
                "    \"engineering\",\n",
                "    \"math\",\n",
                "    \"art\",\n",
                "    \"music\",\n",
                "]\n",
                "challenges = []\n",
                "challenge_id = 0\n",
                "for challenge_topic in challenge_topics:\n",
                "    print(\"=====================\")\n",
                "    print(f\"Generating questions about challenge_topic: {challenge_topic}\")\n",
                "    for i in range(len(create_challenge_prompt_starts_with)):\n",
                "        # create_challenge_prompt = f\"<|im_start|>user\\nAsk a question that can be answered with common knowledge. Do NOT give the answer. Start the question with {create_challenge_prompt_starts_with[i]}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "        create_challenge_prompt = f\"<|im_start|>user\\nAsk a question about {challenge_topic}, that can be answered with common knowledge. Do NOT give the answer. Start the question with {create_challenge_prompt_starts_with[i]}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "        if create_challenge_seed is None:\n",
                "            create_challenge_seed = calculate_hash_seed(create_challenge_prompt)\n",
                "            print(\n",
                "                f\"create_challenge_prompt = {create_challenge_prompt} (seed = {create_challenge_seed})\"\n",
                "            )\n",
                "            print(\"-------------------\")\n",
                "        challenge_question = run_llama_cpp(\n",
                "            create_challenge_prompt,\n",
                "            create_challenge_num_tokens,\n",
                "            create_challenge_seed,\n",
                "            create_challenge_temp,\n",
                "            # create_challenge_top_k,\n",
                "            # create_challenge_top_p,\n",
                "            # create_challenge_min_p,\n",
                "        )\n",
                "\n",
                "        challenge_question = filter_text(challenge_question)\n",
                "\n",
                "        # Continue (skip storing the question) if there is no question mark\n",
                "        if \"?\" not in challenge_question:\n",
                "            continue\n",
                "\n",
                "        # Remove everything after the first question mark\n",
                "        challenge_question = challenge_question.split(\"?\")[0] + \"?\"\n",
                "\n",
                "        create_challenge_seed = calculate_hash_seed(challenge_question)\n",
                "        print(f\"--\\n{challenge_question} (seed = {create_challenge_seed})\")\n",
                "\n",
                "        challenges.append(\n",
                "            {\n",
                "                \"challenge_id\": str(challenge_id),\n",
                "                \"challenge_topic\": challenge_topic,\n",
                "                \"challenge_question\": challenge_question,\n",
                "            }\n",
                "        )\n",
                "        challenge_id += 1\n",
                "\n",
                "        # Continuously save the challenges to a JSON file\n",
                "        # Specify the file path\n",
                "        file_path = \"1-challenges.json\"\n",
                "\n",
                "        # Write the list to a JSON file\n",
                "        with open(file_path, \"w\") as file:\n",
                "            json.dump(challenges, file, indent=4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "PoAIW",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
