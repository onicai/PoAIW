{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Verify we're in the Conda environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "\n",
                "print(sys.executable)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import python packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "from PIL import Image\n",
                "import base64\n",
                "import io\n",
                "from dotenv import load_dotenv\n",
                "import requests\n",
                "import pprint\n",
                "from pathlib import Path\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "import subprocess\n",
                "import jupyter_black\n",
                "\n",
                "# Activate the jupyter_black extension, which reformats code cells with black\n",
                "# https://github.com/n8henrie/jupyter-black\n",
                "jupyter_black.load()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "\n",
                "def filter_text(text):\n",
                "    # Only keep the first line of the answer\n",
                "    text = text.split(\"\\n\")[0]\n",
                "\n",
                "    # Remove quotes from the answer. Both single and double quotes are removed.\n",
                "    text = text.replace('\"', \"\").replace(\"'\", \"\")\n",
                "\n",
                "    # Remove leading and trailing whitespaces\n",
                "    text = text.strip()\n",
                "\n",
                "    # Remove everything after the first space\n",
                "    text = text.split(\" \")[0]\n",
                "\n",
                "    # # Use regex to remove special characters\n",
                "    # text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
                "\n",
                "    # # Remove backslash from the answer.\n",
                "    # text = text.replace(\"\\\\\", \"\")\n",
                "\n",
                "    # # Remove [end of text]\n",
                "    # text = text.replace(\"[end of text]\", \"\")\n",
                "\n",
                "    return text\n",
                "\n",
                "\n",
                "# Example usage\n",
                "text = \"Byzantine\\u5171\\u8bc6 ('Byzantine' \\\"consensus\\\")\\n next line must be removed\"\n",
                "filtered_text = filter_text(text)\n",
                "print(filtered_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define where the llama-cli is located, relative to this notebook\n",
                "LLAMA_CLI_PATH = \"../../../ggml_org_llama_615212.cpp/build/bin/llama-cli\"\n",
                "\n",
                "# Select a model to use\n",
                "MODEL = \"../../../llama_cpp_canister/models/Qwen/Qwen2.5-0.5B-Instruct-GGUF/qwen2.5-0.5b-instruct-q8_0.gguf\"\n",
                "# MODEL = \"../../../llama_cpp_canister/models/tensorblock/SmolLM2-135M-Instruct-GGUF/SmolLM2-135M-Instruct-Q8_0.gguf\"\n",
                "# MODEL = \"../../../llama_cpp_canister/models/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf\"\n",
                "# MODEL = \"../../../llama_cpp_canister/models/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B-Q2_K.gguf\"\n",
                "\n",
                "print_command = True\n",
                "\n",
                "\n",
                "def run_llama_cpp(\n",
                "    prompt,\n",
                "    num_tokens,\n",
                "    seed,\n",
                "    temp,\n",
                "    top_k,\n",
                "    top_p,\n",
                "    min_p,\n",
                "):\n",
                "    command = [\n",
                "        LLAMA_CLI_PATH,\n",
                "        \"-m\",\n",
                "        MODEL,\n",
                "        \"--no-warmup\",  # needed when running from CLI. Is default for llama_cpp_canister\n",
                "        \"-no-cnv\",  # needed when running from CLI. Is default for llama_cpp_canister\n",
                "        \"--simple-io\",\n",
                "        \"--no-display-prompt\",  # only return the generated text, without special characters\n",
                "        # \"-sp\", # output special tokens\n",
                "        \"-n\",\n",
                "        f\"{num_tokens}\",\n",
                "        \"--seed\",\n",
                "        f\"{seed}\",\n",
                "        \"--temp\",\n",
                "        f\"{temp}\",\n",
                "        \"--top-k\",\n",
                "        f\"{top_k}\",\n",
                "        \"--top-p\",\n",
                "        f\"{top_p}\",\n",
                "        \"--min-p\",\n",
                "        f\"{min_p}\",\n",
                "        \"-p\",\n",
                "        prompt,\n",
                "    ]\n",
                "\n",
                "    # print this only once !\n",
                "    global print_command\n",
                "    if print_command:\n",
                "        print_command = False\n",
                "        # Print the command on a single line for terminal use, preserving \\n\n",
                "        print(\n",
                "            \"\\nCommand:\\n\",\n",
                "            f\"{LLAMA_CLI_PATH} -m {MODEL} --no-warmup -no-cnv --simple-io --no-display-prompt -n {num_tokens} --seed {seed} --temp {temp} -p '{prompt}'\".replace(\n",
                "                \"\\n\", \"\\\\n\"\n",
                "            ),\n",
                "        )\n",
                "\n",
                "    # Run the command and capture the output\n",
                "    result = subprocess.run(\n",
                "        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
                "    )\n",
                "    output = result.stdout\n",
                "    return output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import pprint\n",
                "\n",
                "# Specify the file path\n",
                "file_path = \"2-mainer.json\"\n",
                "\n",
                "# Read the list from the JSON file\n",
                "with open(file_path, \"r\") as file:\n",
                "    challenges_with_answers = json.load(file)\n",
                "\n",
                "print(f\"challenges_with_answers have been read from the file: {file_path}\")\n",
                "\n",
                "pprint.pprint(challenges_with_answers[:3])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "judge_num_tokens = 10\n",
                "judge_seed = 42\n",
                "\n",
                "# disable all these\n",
                "judge_temp = 0.0\n",
                "# judge_top_k = 0  # (default: 40, 0 = disabled)\n",
                "# judge_top_p = 1.0  # (default: 0.9, 1.0 = disabled)\n",
                "# judge_min_p = 0.0  # (default: 0.1, 0.0 = disabled)\n",
                "\n",
                "# Defaults for these\n",
                "# judge_temp = 0.0\n",
                "judge_top_k = 40  # (default: 40, 0 = disabled)\n",
                "judge_top_p = 0.9  # (default: 0.9, 1.0 = disabled)\n",
                "judge_min_p = 0.1  # (default: 0.1, 0.0 = disabled)\n",
                "\n",
                "judge = challenges_with_answers.copy()\n",
                "\n",
                "for challenge in judge[0:]:\n",
                "    print_prompt = True\n",
                "    print_prompt_2 = True\n",
                "    challenge_id = challenge[\"challenge_id\"]\n",
                "    challenge_topic = challenge[\"challenge_topic\"]\n",
                "    challenge_question = challenge[\"challenge_question\"]\n",
                "    mainer_answers = challenge[\"mainer_answers\"]\n",
                "\n",
                "    # keep track of scored answers to avoid using the LLM for re-scoring\n",
                "    scored_answers = []\n",
                "    scores = []\n",
                "\n",
                "    for next_mainer_answer in mainer_answers:\n",
                "        filtered_answer = filter_text(next_mainer_answer[\"mainer_answer\"])\n",
                "\n",
                "        print(\"--\")\n",
                "        print(f\"filtered_answer: {filtered_answer}\")\n",
                "\n",
                "        # Get the score if the filtered answer is already scored\n",
                "        if filtered_answer in scored_answers:\n",
                "            judge_score = scores[scored_answers.index(filtered_answer)]\n",
                "            print(f\"Using the existing score: {judge_score}\")\n",
                "            next_mainer_answer[\"judge_score\"] = judge_score\n",
                "            continue\n",
                "\n",
                "        # This answer is not yet scored, so use the LLM to score it\n",
                "        #         judge_prompt = f\"\"\"\n",
                "        # <|im_start|>system\n",
                "        # You must score answers based on its correctness to the question:\n",
                "\n",
                "        # - {challenge_question}\n",
                "\n",
                "        # Score the answer between 1 and 5.\n",
                "        # 1 = completely wrong\n",
                "        # 2 = mostly wrong\n",
                "        # 3 = partially correct\n",
                "        # 4 = mostly correct\n",
                "        # 5 = completely correct\n",
                "\n",
                "        # <|im_end|>\n",
                "        # <|im_start|>user\n",
                "        # Score this answer:\n",
                "\n",
                "        # - {filtered_answer}\n",
                "\n",
                "        # Respond with the score only, nothing else.\n",
                "        # <|im_end|>\n",
                "        # <|im_start|>assistant\n",
                "        # \"\"\"\n",
                "\n",
                "        # GOOD ! but eco. gets a 4\n",
                "        #         judge_prompt = f\"\"\"\n",
                "        # <|im_start|>system\n",
                "        # You grade answers based on its correctness to the question:\n",
                "\n",
                "        # - {challenge_question}\n",
                "\n",
                "        # Grade the answer between 1 and 5.\n",
                "        # 1 = completely wrong\n",
                "        # 2 = mostly wrong\n",
                "        # 3 = partially correct\n",
                "        # 4 = mostly correct\n",
                "        # 5 = completely correct\n",
                "\n",
                "        # <|im_end|>\n",
                "        # <|im_start|>user\n",
                "        # Grade this answer:\n",
                "\n",
                "        # - {filtered_answer}\n",
                "\n",
                "        # Respond with the grade only, nothing else.\n",
                "        # <|im_end|>\n",
                "        # <|im_start|>assistant\n",
                "        # \"\"\"\n",
                "\n",
                "        # GOOD ! and eco. gets a 1 ; parachain gets a 5\n",
                "        judge_prompt = f\"\"\"\n",
                "<|im_start|>system\n",
                "You grade answers based on its correctness to the question: \n",
                "\n",
                "- {challenge_question}\n",
                "\n",
                "Grade the answer between 1 and 5.\n",
                "1 = completely wrong\n",
                "2 = mostly wrong\n",
                "3 = partially correct\n",
                "4 = mostly correct\n",
                "5 = completely correct\n",
                "\n",
                "<|im_end|>\n",
                "<|im_start|>user\n",
                "Grade this answer based on its correctness: \n",
                "\n",
                "- {filtered_answer}\n",
                "\n",
                "Respond with the grade only, nothing else.\n",
                "<|im_end|>\n",
                "<|im_start|>assistant\n",
                "\"\"\"\n",
                "\n",
                "        if print_prompt:\n",
                "            print_prompt = False\n",
                "            print(\"-------------------\")\n",
                "            print(\"The prompt for the judge:\")\n",
                "            print(judge_prompt)\n",
                "            print(\" \")\n",
                "\n",
                "        judge_answer = run_llama_cpp(\n",
                "            judge_prompt,\n",
                "            judge_num_tokens,\n",
                "            judge_seed,\n",
                "            judge_temp,\n",
                "            judge_top_k,\n",
                "            judge_top_p,\n",
                "            judge_min_p,\n",
                "        )\n",
                "\n",
                "        print(f\"judge_answer (raw): {judge_answer}\")\n",
                "\n",
                "        judge_answer = filter_text(judge_answer)\n",
                "\n",
                "        judge_score = int(judge_answer)\n",
                "        print(f\"judge_score: {judge_score}\")\n",
                "\n",
                "        # --------------------------------------------------\n",
                "        # if judge_score == 5:\n",
                "        # # --------------------------------------------------\n",
                "        # print(\"Score is 5 (!) - Ask 1.5B LLM to score it too\")\n",
                "        # judge_answer_2 = run_llama_cpp(\n",
                "        #     judge_prompt,\n",
                "        #     judge_num_tokens,\n",
                "        #     judge_seed,\n",
                "        #     judge_temp,\n",
                "        #     judge_top_k,\n",
                "        #     judge_top_p,\n",
                "        #     judge_min_p,\n",
                "        # )\n",
                "\n",
                "        # print(f\"judge_answer_2 (raw): {judge_answer_2}\")\n",
                "\n",
                "        # # judge_answer_2 = filter_text(judge_answer_2)\n",
                "\n",
                "        # judge_score_2 = int(judge_answer_2)\n",
                "        # print(f\"judge_score: {judge_score_2}\")\n",
                "\n",
                "        # --------------------------------------------------\n",
                "        #         print(\"Score is 5 (!) - Ask LLM if this is indeed a correct answer\")\n",
                "        #         judge_prompt_2 = f\"\"\"\n",
                "        # <|im_start|>system\n",
                "        # You confirm if an answer is completely correct to the question:\n",
                "\n",
                "        # - {challenge_question}\n",
                "\n",
                "        # You answer: yes or no\n",
                "        # <|im_end|>\n",
                "        # <|im_start|>user\n",
                "        # Are you sure this answer is 100% correct?\n",
                "\n",
                "        # - {filtered_answer}\n",
                "\n",
                "        # Respond with yes or no only, nothing else.\n",
                "        # <|im_end|>\n",
                "        # <|im_start|>assistant\n",
                "        # \"\"\"\n",
                "        #         if print_prompt_2:\n",
                "        #             print_prompt_2 = False\n",
                "        #             print(\"-------------------\")\n",
                "        #             print(\"The prompt_2 for the judge:\")\n",
                "        #             print(judge_prompt_2)\n",
                "        #             print(\" \")\n",
                "\n",
                "        #         judge_answer_2 = run_llama_cpp(\n",
                "        #             judge_prompt_2,\n",
                "        #             judge_num_tokens,\n",
                "        #             judge_seed,\n",
                "        #             judge_temp,\n",
                "        #             judge_top_k,\n",
                "        #             judge_top_p,\n",
                "        #             judge_min_p,\n",
                "        #         )\n",
                "\n",
                "        #         print(f\"judge_answer_2 (raw): {judge_answer_2}\")\n",
                "\n",
                "        #         if \"yes\" not in judge_answer_2:\n",
                "        #             judge_score = 4  # Downgrade to score if it is actually not correct\n",
                "\n",
                "        # -----------------------------------------------\n",
                "\n",
                "        #\n",
                "        # Save the score\n",
                "        scored_answers.append(filtered_answer)\n",
                "        scores.append(judge_score)\n",
                "        next_mainer_answer[\"judge_score\"] = judge_score\n",
                "\n",
                "        # On a regular basis, save it, so we can monitor progress\n",
                "        # Specify the file path\n",
                "        file_path = \"3-judge.json\"\n",
                "\n",
                "        # Write the list to a JSON file\n",
                "        with open(file_path, \"w\") as file:\n",
                "            json.dump(judge, file, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "PoAIW",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
